{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f13b193-4b99-4846-8acc-286a9b12691f",
   "metadata": {},
   "source": [
    "<h2>Q - Learning Algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba8ace6-bb70-4462-8faf-2acede921fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Define the grid world -> environment\n",
    "GRID_SIZE = 3\n",
    "START = (0, 0)\n",
    "GOAL = (2, 2)\n",
    "OBSTACLE = (1, 1)\n",
    "\n",
    "# Define Actions -> action space\n",
    "ACTIONS = [\n",
    "    (-1, 0), # up\n",
    "    (0, 1), # right\n",
    "    (1, 0), # down\n",
    "    (0, -1) # left\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbc9e22-c1fc-4a07-9fa1-0b315c3fdcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state: Tuple[int, int]) -> bool:\n",
    "    return (0 <= state[0] < GRID_SIZE and\n",
    "            0 <= state[1] < GRID_SIZE and\n",
    "            state != OBSTACLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead78c28-171b-4fb7-b04b-46c22f1a49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state: Tuple[int, int], action: Tuple[int, int]) -> Tuple[int, int]:\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    return next_state if is_valid_state(next_state) else state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0144c-ab52-4cdd-89e7-7a3a155eb92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ae9444-1678-46f4-a0e7-f2b5cd4a8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Q - Learning parameters\n",
    "EPSILON = 0.3\n",
    "ALPHA = 0.3\n",
    "GAMMA = 0.99\n",
    "EPISODES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7063fe5a-831d-474a-b273-2cd7f2fbf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state: Tuple[int, int], next_state: Tuple[int, int]) -> int:\n",
    "    if next_state == GOAL:\n",
    "        return 100\n",
    "    elif next_state == OBSTACLE or next_state == state:\n",
    "        return -10\n",
    "    else: return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8bdf8b-cf63-4f9d-a2d9-24a3027f17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state: Tuple[int, int], q_table: np.ndarray) -> Tuple[int, int]:\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(q_table[state])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4001a8-e407-4622-a9dd-0f28471b6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Bellman Equation\n",
    "def update_q_table(q_table: np.ndarray, state: Tuple[int, int], action: Tuple[int, int], \n",
    "                   reward: int, next_state: Tuple[int, int]) -> None:\n",
    "    action_idx = ACTIONS.index(action)\n",
    "    q_table[state][action_idx] += ALPHA * (reward + GAMMA * np.max(q_table[next_state]) - q_table[state][action_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a0284-f369-42eb-815a-47cfe79319b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9639a8ff-5bb3-4291-b141-86d5d7146a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_agent() -> np.ndarray:\n",
    "    q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "    \n",
    "    for _ in range(EPISODES):\n",
    "        state = START\n",
    "        while state != GOAL:\n",
    "            action = choose_action(state, q_table)\n",
    "            next_state = get_next_state(state, action)\n",
    "            reward = get_reward(state, next_state)\n",
    "            update_q_table(q_table, state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "# Train the agent\n",
    "q_table = train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3117703-b0f3-49f2-9652-2d1a44069165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ce6ad6-13f1-4160-9750-1d2bcb6d4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a final Q-Table representing our trained agent, we can visualize what was learned during the training\n",
    "\n",
    "def visualize_q_table_as_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the Q-table as a grid with all action values for each state.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nDetailed Q-table Grid:\")\n",
    "    \n",
    "    # Header\n",
    "    header = \"   |\" + \"|\".join(f\"   ({i},{j})   \" for i in range(GRID_SIZE) for j in range(GRID_SIZE)) + \"|\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for action_idx, action_symbol in enumerate(action_symbols):\n",
    "        row = f\" {action_symbol} |\"\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if (i, j) == GOAL:\n",
    "                    cell = \"   GOAL    \"\n",
    "                elif (i, j) == OBSTACLE:\n",
    "                    cell = \" OBSTACLE  \"\n",
    "                else:\n",
    "                    q_value = q_table[i, j, action_idx]\n",
    "                    cell = f\" {q_value:9.2f} \"\n",
    "                row += cell + \"|\"\n",
    "        print(row)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "def visualize_best_actions_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the best action and its Q-value for each state in a grid.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nBest Actions Grid:\")\n",
    "    header = \"-\" * (14 * GRID_SIZE + 1)\n",
    "    print(header)\n",
    "\n",
    "    for i in range(GRID_SIZE):\n",
    "        row = \"| \"\n",
    "        for j in range(GRID_SIZE):\n",
    "            if (i, j) == GOAL:\n",
    "                cell = \"   GOAL    \"\n",
    "            elif (i, j) == OBSTACLE:\n",
    "                cell = \" OBSTACLE  \"\n",
    "            else:\n",
    "                best_action_idx = np.argmax(q_table[i, j])\n",
    "                best_q_value = q_table[i, j, best_action_idx]\n",
    "                cell = f\"{action_symbols[best_action_idx]}:{best_q_value:7.2f}  \"\n",
    "            row += cell + \" | \"\n",
    "        print(row)\n",
    "        print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34815714-dc75-4b53-a36f-1427b4fb9f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Q-table Grid:\n",
      "   |   (0,0)   |   (0,1)   |   (0,2)   |   (1,0)   |   (1,1)   |   (1,2)   |   (2,0)   |   (2,1)   |   (2,2)   |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " ^ |     83.12 |     85.06 |     87.02 |     92.12 | OBSTACLE  |     96.02 |     94.06 |     89.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " > |     94.06 |     96.02 |     87.02 |     85.06 | OBSTACLE  |     89.00 |     98.00 |    100.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " v |     94.06 |     85.06 |     98.00 |     96.02 | OBSTACLE  |    100.00 |     87.02 |     89.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " < |     83.12 |     92.12 |     94.06 |     85.06 | OBSTACLE  |     89.00 |     87.02 |     96.02 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Best Actions Grid:\n",
      "-------------------------------------------\n",
      "| >:  94.06   | >:  96.02   | v:  98.00   | \n",
      "-------------------------------------------\n",
      "| v:  96.02   |  OBSTACLE   | v: 100.00   | \n",
      "-------------------------------------------\n",
      "| >:  98.00   | >: 100.00   |    GOAL     | \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Visualize the Q-table as a grid\n",
    "visualize_q_table_as_grid(q_table)\n",
    "\n",
    "# Visualize the best actions and their Q-values in a grid\n",
    "visualize_best_actions_grid(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b227174-c358-46e0-82fc-f360456bd9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f49a37-f4b0-4635-9b3c-c8570f58948f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133a39f-7e27-486a-9de3-6aa64464819d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81afbef1-3c82-447d-a5d2-f27eb22525cb",
   "metadata": {},
   "source": [
    "<h2>DQN Algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bf84381-f190-4800-ba6d-6987f62ff828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b3640be-fb9e-4112-9fdf-2156928e3ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Size:  4\n",
      "Action Size:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"State Size: \", state_size)\n",
    "print(\"Action Size: \", action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810e0474-a724-44be-b391-7dd8ebe1820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Model):\n",
    "    def __init__(self, action_size, **kwargs):\n",
    "        super(DQN, self).__init__(**kwargs)\n",
    "        self.action_size = action_size\n",
    "        self.d1 = layers.Dense(24, activation='relu', name='d1')\n",
    "        self.d2 = layers.Dense(24, activation='relu', name='d2')\n",
    "        self.d3 = layers.Dense(action_size, activation='linear', name='d3')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return self.d3(x)\n",
    "\n",
    "    # Configs for loading the saved model file later on\n",
    "    def get_config(self):\n",
    "        config = super(DQN, self).get_config()\n",
    "        config.update({\"action_size\": self.action_size})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c8e0e3b-d656-43dc-b432-c7d3677112bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay memory deque\n",
    "memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe6342c6-e545-483b-87b0-8b22c10ae545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "    def _build_model(self):\n",
    "        return DQN(self.action_size)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model(np.array([state]))\n",
    "        return np.argmax(q_values[0].numpy())\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        self.model.save(filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        # Load the saved model from the specified filepath\n",
    "        self.model = tf.keras.models.load_model(filepath, custom_objects={\"DQN\": DQN})\n",
    "        self.target_model = tf.keras.models.load_model(filepath, custom_objects={\"DQN\": DQN})\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = self.model(np.array([state]), training=True)\n",
    "                q_value = q_values[0][action]\n",
    "\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    next_action = np.argmax(self.model(np.array([next_state]))[0].numpy())\n",
    "                    t = self.target_model(np.array([next_state]))[0][next_action]\n",
    "                    target = reward + self.gamma * t\n",
    "\n",
    "                loss = tf.reduce_mean(tf.square(target - q_value))\n",
    "\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fff7d5f-ef1c-42ed-ab2d-710ce2e81712",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32           # Number of samples used for each training step\n",
    "n_episodes = 100          # Total number of episodes to train on\n",
    "gamma = 0.95              # Discount factor for future rewards (0 to 1)\n",
    "epsilon = 1.0             # Initial exploration rate (1 = 100% random actions)\n",
    "epsilon_min = 0.01        # Minimum exploration rate\n",
    "epsilon_decay = 0.995     # Decay factor for epsilon after each episode\n",
    "learning_rate = 0.001     # Step size for neural network weight updates\n",
    "update_target_every = 5   # Number of episodes between target network updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc15f57-5fe2-49e5-99dd-1b6fb3608aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/100, Score: 22, Epsilon: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cartpole_model/model_0.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m         agent\u001b[38;5;241m.\u001b[39mupdate_target_model()\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m         agent\u001b[38;5;241m.\u001b[39msave_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     41\u001b[0m agent\u001b[38;5;241m.\u001b[39msave_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_500.keras\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[17], line 33\u001b[0m, in \u001b[0;36mAgent.save_model\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(filepath)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:143\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, weights_format, zipped)\u001b[0m\n\u001b[0;32m    141\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(zip_filepath\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    144\u001b[0m         _save_model_to_fileobj(model, f, weights_format)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cartpole_model/model_0.keras'"
     ]
    }
   ],
   "source": [
    "output_dir = './cartpole_model/'\n",
    "\n",
    "# Initialize the Agent\n",
    "agent = Agent(state_size, action_size, gamma=gamma, epsilon=epsilon, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, learning_rate=learning_rate)\n",
    "done = False\n",
    "\n",
    "# Main Script\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()[0]\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    for time_t in range(500):\n",
    "        action = agent.act(state[0])\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        done = done or truncated\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state[0], action, reward, next_state[0], done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode: {e}/{n_episodes}, Score: {time_t}, Epsilon: {agent.epsilon:.2f}\")\n",
    "            break\n",
    "\n",
    "    if len(memory) > batch_size:\n",
    "        loss = agent.replay(batch_size)\n",
    "\n",
    "    # Update epsilon\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "    # Update target network\n",
    "    if e % update_target_every == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        agent.save_model(os.path.join(output_dir, f'model_{e}.keras'))\n",
    "\n",
    "agent.save_model(os.path.join(output_dir, f'model_500.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913f949-236d-4b08-9e05-00bedd835984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
